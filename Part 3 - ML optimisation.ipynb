{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "working-significance",
   "metadata": {},
   "source": [
    "#### Machine learning optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multivariate linear regression using train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying train test split and linear regression\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/insurance_charges.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['charges', 'insuranceclaim'], axis=1)\n",
    "y = df['charges']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train shape: {}, X_test shape: {}, y_train shape: {}, y_test shape {}\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try printing out the shapes of these splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-royalty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions_train = model.predict(X_train)\n",
    "mean_squared_error(y_train, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-cycle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.predict(X_test)\n",
    "mean_squared_error(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-devices",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the errors between the two. What do you notice? What's the significance of either being significantly different\n",
    "# from the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-determination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Carry out the same process for the Boston house dataset, X and y already provided.\n",
    "Output the scores for train and test sets\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "df = load_boston()\n",
    "df = pd.DataFrame(np.c_[df['data'], df['target']],\n",
    "                  columns= np.append(df['feature_names'], ['target']))\n",
    "y = df['target']\n",
    "X = df.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "violent-bandwidth",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-oakland",
   "metadata": {},
   "source": [
    "#### Standardising our dataset\n",
    "Note: make sure to fit on the training set and then apply this to both. Don't include test in the fit (this helps to prevent data leakage from our test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the X_train, X_test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing our data first\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the standardised X_train/X_test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-wesley",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "average-candidate",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-nickname",
   "metadata": {},
   "source": [
    "#### L1 / L2 reglularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your train/test split below\n",
    "df = pd.read_csv(\"datasets/insurance_charges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-response",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change to L2 penalty simple change penalty='l2'\n",
    "# Try this model with L1/L2 and different values of C (C=inverse alpha)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='saga',C=0.1, max_iter=10000)\n",
    "# model = LogisticRegression(penalty='l1', solver='saga',C=0.01)\n",
    "# model = LogisticRegression(penalty='l1', solver='saga',C=0.00001)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Output the performance below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have an in-depth play with  using L1 and L2 with different solvers and different values of C\n",
    "# There are so many possible values we could use for C. It might be helpful to do a 'scan' of a sensible range of C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Creating a function for alpha tuning\n",
    "\n",
    "- Create a function which can:\n",
    "- take in a list of possible alpha values\n",
    "- run these through an L1 logistic regression\n",
    "- store the roc_auc_scores\n",
    "- at the end plot all of these scores to show us the optimal L1 value\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-translation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One solution\n",
    "def find_alpha(alphas):\n",
    "    roc_aucs = []\n",
    "    for k in range(len(alphas)):\n",
    "        alpha = alphas[k]\n",
    "        model = LogisticRegression(penalty='l1',random_state=1,solver='saga',max_iter=1000,C=alpha)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        '''\n",
    "        Include performance score here\n",
    "        '''\n",
    "    plt.plot(alphas, roc_aucs)\n",
    "    plt.xscale('log')\n",
    "    plt.title('Performance vs alpha')\n",
    "    plt.show()\n",
    "\n",
    "find_alpha(alphas = [0.0001,0.001,0.015, 0.02, 0.025, 0.03,0.035,0.04,1,10,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-yacht",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-marble",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
